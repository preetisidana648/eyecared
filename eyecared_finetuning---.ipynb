{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# data training\n"
      ],
      "metadata": {
        "id": "_JKhdnqq4aZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "op5PUfB0Alqm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "# fourbit_models = [\n",
        "#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "#     \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "#     \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "#     \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "#     \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
        "#     \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "# ] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "75a7a80209c14da1a3224b217913769e",
            "70a34a7daa534ab588cfbf6ca86aa2a0",
            "c1ff5bd975af489ead1d1ed3f8286e50",
            "a085e19418be46e2b096543067763038",
            "04ab2060bffb452c9145ecf1ef1eac2f",
            "1b80cc973919401abcb9112f04f48891",
            "dddbd28f15b542a6b56fb9445d07e60a",
            "e23dd4108d5c4b6f966694e2f79c9c57",
            "cd6f5d5580b7417b926bd21fbc368ed7",
            "4b0ed8901ced4e18a8311d43d0313625",
            "9025dd560fad4b1ba590bc8781914722",
            "938c2c0c9e6549c6ac65af6df42e05ff",
            "e5889fca61644df297e1e612c70a687d",
            "3c8250acb6854b5e8a1fab1ebbd2e8b1",
            "410c6a55e03e4420971b25b8dce9b760",
            "0802cab2b2524e8bbefc7c8be750f4c4",
            "e6c0e1b5ac4d4f19bf6950e95e8448e5",
            "181b364818814712aedd2bf490f41b80",
            "f23c0230616543b2aab5adf5d9b8d8fe",
            "2c246ae67e4c43ba821f6f215c35a1f4",
            "64df9833c5e4425c833979bac08f41ea",
            "cc7756e093984f1eb4a4d55714297aa4",
            "cce8a690feab4554b9fa21d56455fb0a",
            "85a9926ae5d34b39b267569e32bb3587",
            "0d54e5fc334641bea3f047aa07163e5f",
            "36935af601ae4497a235cb353a4424a2",
            "501cdded0b0a4591a925e624c8c73b34",
            "92b22764a6274d4580228cb02445b834",
            "ba0c09fd2d964a4698f3c0cbd4132209",
            "1573485ef0fe4ee5adf6aad4fe21d6da",
            "4bb2a569131d4f7d96254bfc993610ba",
            "ae7281421a894beba1563809ff40ed6e",
            "9145b7901b3448c79b6fe496b7de0b66",
            "52ddcc11fe8e4037abe067e9d63124f3",
            "78172a3ad74a494fa0adae45101255eb",
            "77a81e278c974505bd8488474ba528b9",
            "69e729a137bd49bc9c4c9f779047ddc4",
            "3ff2a8114f56487d982f384c0907c05f",
            "fb184dae298442a590e7a1c67df8951e",
            "e25e28867ae64f63ac0c3103d6ea511a",
            "e1c6600d341647a2baaedbfc6e1f6981",
            "5120a877524b423bb0a70af935022294",
            "deb0489cea5d465ba3fe1ddbb2b70078",
            "ea77d2dcd68540258bf747f24923c95c",
            "f0526b7fe64b47b9bd5a85589822e4fa",
            "1c3fdd4af80b4cd4a3624cba81d2485f",
            "458a43b14ad944f99f0d2226e9d63211",
            "5dae471111b1433c98b3bfc1c96cdd14",
            "7c71428d35bf45c698b567042423c31c",
            "fc67388d1b22498bbe7eef0446806264",
            "ba433908a6c546c8a1e9483d5479cb53",
            "c9d9e27b13ea4b9ebbc37097bdf4887b",
            "8e5a8f1e61034cfe9e7212803d0648ed",
            "f491561c6a184c3aaba44b3d15d71ee0",
            "210e33fa443648cca8f5eb24ae8ad139"
          ]
        },
        "id": "0u3uQMv_Ayio",
        "outputId": "0add93ff-b869-4c5d-ca73-0c35cae7fd98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75a7a80209c14da1a3224b217913769e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "938c2c0c9e6549c6ac65af6df42e05ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cce8a690feab4554b9fa21d56455fb0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52ddcc11fe8e4037abe067e9d63124f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0526b7fe64b47b9bd5a85589822e4fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHcbZ_h7A1Hy",
        "outputId": "40ef3b79-8f46-4677-e2ef-33a320db10de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Step 1: Load the CSV file into a pandas DataFrame\n",
        "csv_file_path = \"/content/drive/MyDrive/datatraining/eyecare_appointment_training_data.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Convert the DataFrame to a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Step 2: Define your chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3\",  # Replace with your desired template\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # Mapping as per your data format\n",
        ")\n",
        "\n",
        "# Step 3: Define the formatting function\n",
        "def formatting_prompts_func(examples):\n",
        "    # Convert each row of the DataFrame to the required format\n",
        "    convos = []\n",
        "    for i in range(len(examples['Human Message 1'])):\n",
        "        convo = [\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 1'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 1'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 2'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 2'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 3'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 3'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 4'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 4'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 5'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 5'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 6'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 6'][i]},\n",
        "            {\"from\": \"human\", \"value\": examples['Human Message 7'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 8'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 9'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 10'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 11'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 12'][i]},\n",
        "            {\"from\": \"gpt\", \"value\": examples['Chatbot Response 13'][i]},\n",
        "        ]\n",
        "        convos.append(convo)\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Step 4: Apply the formatting function to the dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Now the dataset is ready for training with the formatted prompts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "75c42028f3ed46bc9f46487fa4cc1b60",
            "585839beadc94981be224446d3ee114a",
            "5f42aa99dbe544a0808af97d28f983ef",
            "1d3b3754dd3949fbb15bc1d1ad63b14e",
            "4f4c205494fb4cd882f324ca6475dd02",
            "721d317bb9b644bf9b9c1861189a95de",
            "7ea43dfad9b2461aaf48179e2a33baa2",
            "a142fd6cc228494aad33a81378c9350c",
            "89613703116a4e34b260884351976428",
            "e6481fc4ef81450abc575d03a118d877",
            "94594e23935f4249b571ff5cac8b3aee"
          ]
        },
        "id": "t5pnVIUPA6I1",
        "outputId": "cd1ae986-aaba-4c8e-e642-5c3715940838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75c42028f3ed46bc9f46487fa4cc1b60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWwUxWdnB-xe",
        "outputId": "9c2a8d10-b84f-4dee-fe21-ff5959786e7b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Human Message 1': \"Hello, this is Olivia Brown calling to book an eye examination appointment for my child at the eyecare hospital. My child has been experiencing some difficulty seeing the board at school. Here are the details:\\nParent's Name: Olivia Brown\\nChild's Name: Lily Brown\\nDOB: June 1, 2012\\nPreferred Appointment Date: next tuesday\\nPreferred Time: Afternoon, if possible\\nReason for Visit: Difficulty seeing the board at school\\nPlease call me back at your earliest convenience to confirm the appointment or suggest an alternative time if my preferred slot is not available. You can reach me at the contact number provided above. Thank you.\\nBest regards,\\nOlivia Brown\\n(555) 789-0123\",\n",
              " 'Chatbot Response 1': 'Ok, thank you for sharing the details. Can you please help provide an email address and Date Of Birth?',\n",
              " 'Human Message 2': 'test@first-insight.com , DOB is December 5, 2000',\n",
              " 'Chatbot Response 2': 'Can you please confirm the following contact details\\nPhone Number: (555) 789-0123\\nEmail: test@first-insight.com\\nDOB: 12-05-2000',\n",
              " 'Human Message 3': \"Yes it's correct\",\n",
              " 'Chatbot Response 3': 'Thanks for the details, We have sent you the OTP on the registered email and Phone Number, Can you please enter the OTP',\n",
              " 'Human Message 4': '1111',\n",
              " 'Chatbot Response 4': 'Thanks for the validation, Please choose providers from the list:\\n? Dr. Emily Thompson\\n? Dr. Mark Davis\\n? Dr. Susan Lee',\n",
              " 'Human Message 5': \"I'll go with Dr. Emily Thompson.\",\n",
              " 'Chatbot Response 5': 'Ok, thank you for sharing the details. Let me check the availability of the appointment. I see the following open slots on June 18, 2024: 2:00 PM, 2:20 PM, 3:00 PM.',\n",
              " 'Human Message 6': \"Let's book the 2:00 PM appointment.\",\n",
              " 'Chatbot Response 6': \"Alright! Can you please confirm the following details once:\\nPatient Information:\\nParent's Name: Olivia Brown\\nChild's Name: Lily Brown\\nDate of Birth: December 5, 2000\\nContact Number: (555) 789-0123\\nAppointment Details:\\nDate: June 18, 2024\\nTime: 2:00 PM\\nProvider: Dr. Emily Thompson\\nReason for Visit: Difficulty seeing the board at school\",\n",
              " 'Human Message 7': 'Yes, they are correct!',\n",
              " 'Chatbot Response 7': 'Your Appointment is scheduled, Thanks for choosing eyecare location! Is there anything I can help you with?',\n",
              " 'Human Message 8': 'Nothing as of now.',\n",
              " 'Chatbot Response 8': 'Thanks, Have a great day!',\n",
              " 'Human Message 9': None,\n",
              " 'Chatbot Response 9': None,\n",
              " 'Human Message 10': None,\n",
              " 'Chatbot Response 10': None,\n",
              " 'Human Message 10.1': None,\n",
              " 'Chatbot Response 10.1': None,\n",
              " 'Human Message 11': None,\n",
              " 'Chatbot Response 11': None,\n",
              " 'Human Message 12': None,\n",
              " 'Chatbot Response 12': None,\n",
              " 'Human Message 13': None,\n",
              " 'Chatbot Response 13': None,\n",
              " 'text': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHello, this is Olivia Brown calling to book an eye examination appointment for my child at the eyecare hospital. My child has been experiencing some difficulty seeing the board at school. Here are the details:\\nParent's Name: Olivia Brown\\nChild's Name: Lily Brown\\nDOB: June 1, 2012\\nPreferred Appointment Date: next tuesday\\nPreferred Time: Afternoon, if possible\\nReason for Visit: Difficulty seeing the board at school\\nPlease call me back at your earliest convenience to confirm the appointment or suggest an alternative time if my preferred slot is not available. You can reach me at the contact number provided above. Thank you.\\nBest regards,\\nOlivia Brown\\n(555) 789-0123<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOk, thank you for sharing the details. Can you please help provide an email address and Date Of Birth?<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ntest@first-insight.com , DOB is December 5, 2000<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nCan you please confirm the following contact details\\nPhone Number: (555) 789-0123\\nEmail: test@first-insight.com\\nDOB: 12-05-2000<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYes it's correct<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThanks for the details, We have sent you the OTP on the registered email and Phone Number, Can you please enter the OTP<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n1111<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThanks for the validation, Please choose providers from the list:\\n? Dr. Emily Thompson\\n? Dr. Mark Davis\\n? Dr. Susan Lee<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI'll go with Dr. Emily Thompson.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOk, thank you for sharing the details. Let me check the availability of the appointment. I see the following open slots on June 18, 2024: 2:00 PM, 2:20 PM, 3:00 PM.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nLet's book the 2:00 PM appointment.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAlright! Can you please confirm the following details once:\\nPatient Information:\\nParent's Name: Olivia Brown\\nChild's Name: Lily Brown\\nDate of Birth: December 5, 2000\\nContact Number: (555) 789-0123\\nAppointment Details:\\nDate: June 18, 2024\\nTime: 2:00 PM\\nProvider: Dr. Emily Thompson\\nReason for Visit: Difficulty seeing the board at school<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYes, they are correct!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThanks, Have a great day!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNone<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNone<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNone<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNone<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNone<|eot_id|>\"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[5][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0wQH5XLCIUH",
        "outputId": "a96df915-1dd9-423f-8ac3-b1c49b7d5ae5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Hello, this is Anthony Poole calling to book an eye examination appointment for my child at the eyecare hospital. My child, Karen, has been experiencing complaints about distant vision. Here are the details: Parent's Name: Anthony Poole, Child's Name: Karen, DOB: August 19, 2014, Preferred Appointment Date: August 27, 2024, Preferred Time: Afternoon, Reason for Visit: Complaints about distant vision.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Ok, thank you for sharing the details. Can you please help provide an email address and Date Of Birth?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "stevensrobyn@example.net, DOB is January 04, 1990<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Can you please confirm the following contact details? Phone Number: 793-632-2247x29006 Email: stevensrobyn@example.net DOB: January 04, 1990<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Yes, it's correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Thanks for the details. We have sent you the OTP on the registered email and Phone Number. Can you please enter the OTP?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "8075<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<API_CALL:sndotp(token, FirstName='Anthony', LastName='Poole', PhoneNumber='793-632-2247x29006', DOB='January 04, 1990', Email='stevensrobyn@example.net')><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "<Get List of Providers><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Thanks for the validation. Please choose providers from the list: Dr. Emily Thompson, Dr. Mark Davis, Dr. Susan Lee<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "I'll go with Dr. Mark Davis.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<API_CALL:get_providers(token, location_id)><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "<Check for available slots with provided preference><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Alright! Can you please confirm the following details once: Parent's Name: Anthony Poole, Child's Name: Karen, Date of Birth: January 04, 1990, Contact Number: 793-632-2247x29006, Appointment Details: Date: August 27, 2024, Time: 10:00 AM, Provider: Dr. Mark Davis, Reason for Visit: Complaints about distant vision.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<API_CALL:book_appointment(token, open_slot_id, appointment_date='August 27, 2024', from_date, reason_id, FirstName='Anthony', LastName='Poole', DOB='January 04, 1990', PhoneNumber='793-632-2247x29006', Email='stevensrobyn@example.net')> Your Appointment is scheduled, Thanks for choosing eyecare location! Is there anything I can help you with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Thanks, Have a great day!<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsloth_template = \\\n",
        "    \"{{ bos_token }}\"\\\n",
        "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
        "    \"{% for message in messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}\"\\\n",
        "        \"{{ '>>> Assistant: ' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "unsloth_eos_token = \"eos_token\"\n",
        "\n",
        "if False:\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        "    )"
      ],
      "metadata": {
        "id": "-f_SvldACNrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 7,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"/content/drive/MyDrive/datatraining\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e53eb4f5cfa941758e6a4589f1743ccc",
            "57be90309a0249c98ffc92a32f3a4611",
            "c7356dff15084e358336a769e02ce320",
            "fdae3f87294f4f2bbae36b217e978f5a",
            "46c4b81c457446e1a2de1ad203f47eb4",
            "10b90bec85ba433cb3664420572ec470",
            "597910d8dce34c509c8d10a250652095",
            "cf56831fae274b5b9d7a7f12ef81ae1f",
            "b92a31bcd5bf41179abdcfea1d8ab5ac",
            "ec8e19e13eb54e6b83d7c1668f4e2e2a",
            "3eae3addbdfb4d18b810124fdd9481e7"
          ]
        },
        "id": "14zsH6cZCQUQ",
        "outputId": "f9817c52-1d57-4745-f991-b32679c9438f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e53eb4f5cfa941758e6a4589f1743ccc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pTTwdrh1CTmq",
        "outputId": "f7e31977-d7f0-4a1f-b82a-05739423d115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 65 | Num Epochs = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 7 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 28 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 38:18, Epoch 24/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.871000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.927800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.937100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.101300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.720500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.870700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.740300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.534900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.338500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.239600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.165400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.081700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.079100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.963000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.815500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.855400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.752400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.825500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.730200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.594500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.562900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.706100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.491500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.455100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.482300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.398000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.376200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.306800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.305500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.286900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.332800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.298500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.242100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.255000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.219100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.224200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.231500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.199800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.196900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.204100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.212900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.213100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.151400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.154900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.155900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.210300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.159500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.165400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.167700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.143200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.165400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU_bOssfEPkk",
        "outputId": "2d6a518b-a759-408f-f870-4f39cb1512f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "6.699 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Axh4ZR2CwPm",
        "outputId": "00c39c44-5f4b-4a95-9ad3-13abf3b86fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "616.2519 seconds used for training.\n",
            "10.27 minutes used for training.\n",
            "Peak reserved memory = 6.699 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 45.423 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"i want to book appointment\"}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OO4IxKC01D",
        "outputId": "61e70d1b-bd20-45aa-d7f2-b3cea7737fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\ni want to book appointment<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nPlease provide the following details:\\n\\n1. Name\\n2. DOB (Date of Birth)\\n3. Preferred Appointment Date\\n4. Preferred Time (Morning/Afternoon)\\n5. Reason for Visit (Select one):\\n\\t* Eye Check-up\\n\\t* Eye Problem\\n\\t* Eye Comfort\\n\\t* Eye Surgery']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Initialize the tokenizer with the chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3\",  # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Initialize conversation list\n",
        "messages = []\n",
        "max_sequence_length = 2048\n",
        "# Conversation loop\n",
        "while True:\n",
        "    # Get input from the user (human message)\n",
        "    human_input = input(\"You: \")\n",
        "\n",
        "    # Add the human message to the conversation\n",
        "    messages.append({\"from\": \"human\", \"value\": human_input})\n",
        "\n",
        "    # Apply the chat template, tokenize, and prepare for generation\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Must add for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    if inputs.size(1) > max_sequence_length:\n",
        "        inputs = inputs[:, -max_sequence_length:]\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "    # Decode the generated output\n",
        "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Extract the chatbot's response and add to the conversation\n",
        "    chatbot_response = decoded_output[0]\n",
        "\n",
        "    # Print the chatbot's response\n",
        "    print(f\"ChatBot: {chatbot_response}\")\n",
        "    messages.append({\"from\": \"gpt\", \"value\": chatbot_response})\n",
        "\n",
        "    # Check if the conversation should continue or end\n",
        "    if human_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Ending the conversation. Goodbye!\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1egAWztOYpG",
        "outputId": "18ba848a-4b6b-4a04-9771-85600e3b67db",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?\n",
            "ChatBot: user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email address\n",
            "You: 555123456 test@first-insight.com 3rd Aug\n",
            "ChatBot: user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?\n",
            "You: 1998\n",
            "ChatBot: user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "Great, Thanks for the details, We have sent you the OTP on the registered email and Phone Number, please enter the OTP\n",
            "You: 1111\n",
            "ChatBot: user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "Great, Thanks for the details, We have sent you the OTP on the registered email and Phone Number, please enter the OTPuser\n",
            "\n",
            "1111assistant\n",
            "\n",
            "Thank you for the details, your Order No. 111 is ready for Pickup <<API Response>>, Below details are shown as per our records:\n",
            "Phone Number: 555123456\n",
            "Email: test@first-insight.com\n",
            "DOB: 03/08/1998\n",
            "You: I want to know my appointment time for today, seems I might not be able to make it\n",
            "ChatBot: user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "Great, Thanks for the details, We have sent you the OTP on the registered email and Phone Number, please enter the OTPuser\n",
            "\n",
            "1111assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "user\n",
            "\n",
            "Hello This is John Doe, I ordered glasses on my last Visit, Can I pick up the glasses and contacts on Thursday around 8 am?assistant\n",
            "\n",
            "Ok sure, Can you please help provide Date of Birth, phone number or email addressuser\n",
            "\n",
            "555123456 test@first-insight.com 3rd Augassistant\n",
            "\n",
            "Thank you for the details, can you please provide your birth Year?user\n",
            "\n",
            "1998assistant\n",
            "\n",
            "Great, Thanks for the details, We have sent you the OTP on the registered email and Phone Number, please enter the OTPuser\n",
            "\n",
            "1111assistant\n",
            "\n",
            "Thank you for the details, your Order No. 111 is ready for Pickup <<API Response>>, Below details are shown as per our records:\n",
            "Phone Number: 555123456\n",
            "Email: test@first-insight.com\n",
            "DOB: 03/08/1998user\n",
            "\n",
            "I want to know my appointment time for today, seems I might not be able to make itassistant\n",
            "\n",
            "Sure, Can you please help provide your Name, DOB, Phone Number or Email?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/datatraining/checkpoint-60/README.md')\n",
        "files.download('/content/drive/MyDrive/datatraining/checkpoint-60/adapter_config.json')\n",
        "files.download('/content/drive/MyDrive/datatraining/checkpoint-60/adapter_model.safetensors')\n",
        "files.download('/content/drive/MyDrive/datatraining/checkpoint-60/adapter_model.safetensors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CB66Djsw8AFF",
        "outputId": "f6900c11-05e8-4da3-f308-89b0fac2e376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1dadf97c-ce28-46ca-b7f8-2a791b94a49c\", \"README.md\", 5110)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b400ddb3-2f97-4246-8823-99e469488613\", \"adapter_config.json\", 736)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_752e79e2-1053-4715-b78a-b9bd0eb9cf5e\", \"adapter_model.safetensors\", 167832240)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5ae7c4e7-e3a6-4b8e-8593-5ec5f2490ac0\", \"adapter_model.safetensors\", 167832240)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r dataset.zip /content/drive/MyDrive/datatraining"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U_Os-e9ktljM",
        "outputId": "7506e698-b89b-4749-ddc7-abe599c543c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/drive/MyDrive/datatraining/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/datatraining/eyecare_appointment_training_data.csv (deflated 87%)\n",
            "  adding: content/drive/MyDrive/datatraining/runs/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/datatraining/runs/Aug21_02-31-47_97386205e7bd/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/datatraining/runs/Aug21_02-31-47_97386205e7bd/events.out.tfevents.1724207517.97386205e7bd.915.0 (deflated 65%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/README.md (deflated 66%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/adapter_config.json (deflated 54%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/special_tokens_map.json (deflated 70%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/tokenizer.json (deflated 74%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/training_args.bin (deflated 51%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/optimizer.pt (deflated 15%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/scheduler.pt (deflated 56%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/rng_state.pth (deflated 25%)\n",
            "  adding: content/drive/MyDrive/datatraining/checkpoint-60/trainer_state.json (deflated 80%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/')\n"
      ],
      "metadata": {
        "id": "jD7bYzS1t_8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model load"
      ],
      "metadata": {
        "id": "fyGDDNWKRwdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDmOIDcIU0ia",
        "outputId": "8f1e34e7-0ff5-43f8-81d6-e27224b5015a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import torch\n",
        "# Specify the path to the directory where the model and tokenizer files are saved\n",
        "model_directory = \"/content/drive/MyDrive/checkpoint-60\"\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
        "\n",
        "# Initialize the tokenizer with the chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3\",  # Choose the template style that fits your use case\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
        ")\n",
        "\n",
        "# Load the model from the directory and set the precision to float16\n",
        "model = AutoModelForCausalLM.from_pretrained(model_directory, torch_dtype=torch.float16, trust_remote_code=True)\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iumyi3vOVnVe",
        "outputId": "c6b29f72-04cb-49c3-acde-c557512c9eb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (up_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (down_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize conversation list\n",
        "messages = []\n",
        "max_sequence_length = 2048\n",
        "\n",
        "# Conversation loop\n",
        "while True:\n",
        "    # Get input from the user (human message)\n",
        "    human_input = input(\"You: \")\n",
        "\n",
        "    # Add the human message to the conversation\n",
        "    messages.append({\"from\": \"human\", \"value\": human_input})\n",
        "\n",
        "    # Apply the chat template, tokenize, and prepare for generation\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Must add for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Truncate the input to fit within the model's max sequence length\n",
        "    if inputs.size(1) > max_sequence_length:\n",
        "        inputs = inputs[:, -max_sequence_length:]\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "    # Decode the generated output\n",
        "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Extract the chatbot's response and add to the conversation\n",
        "    chatbot_response = decoded_output[0]\n",
        "\n",
        "    # Print the chatbot's response\n",
        "    print(f\"ChatBot: {chatbot_response}\")\n",
        "    messages.append({\"from\": \"gpt\", \"value\": chatbot_response})\n",
        "\n",
        "    # Check if the conversation should continue or end\n",
        "    if human_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Ending the conversation. Goodbye!\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fgdqZIRGXZgS",
        "outputId": "7ebfe77d-423c-4717-d7b5-0c5452e3c6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel it\n",
            "ChatBot: user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?\n",
            "You: My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistant\n",
            "ChatBot: user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "Thanks for the details, We have sent the OTP on your mobile number to verify your identity. Can you enter OTP?\n",
            "You: 1111\n",
            "ChatBot: user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "Thanks for the details, We have sent the OTP on your mobile number to verify your identity. Can you enter OTP?user\n",
            "\n",
            "1111assistant\n",
            "\n",
            "Yes, your appointment is scheduled tomorrow at 10 AM, Are you sure you want to cancel it? Or do you want to reschedule it?\n",
            "You: cancel it\n",
            "ChatBot: user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "Thanks for the details, We have sent the OTP on your mobile number to verify your identity. Can you enter OTP?user\n",
            "\n",
            "1111assistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "user\n",
            "\n",
            "I have an appointment tomorrow with Dr Clark, but something came up, and I need to cancel itassistant\n",
            "\n",
            "Sure, Can you please let me know your Name, DOB and Phone Number or Email?user\n",
            "\n",
            "My name is John Doe, DOB: 10th Feb 1990, My mobile number is 3333333333.assistantassistant\n",
            "\n",
            "Thanks for the details, We have sent the OTP on your mobile number to verify your identity. Can you enter OTP?user\n",
            "\n",
            "1111assistant\n",
            "\n",
            "Yes, your appointment is scheduled tomorrow at 10 AM, Are you sure you want to cancel it? Or do you want to reschedule it?user\n",
            "\n",
            "cancel itassistant\n",
            "\n",
            "Sure, your appointment is cancelled, Please visit our site again to book the appointment once you are available. Is there anything else I can help you with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CcgD-rLxf5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}